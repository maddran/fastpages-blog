{
  
    
        "post0": {
            "title": "Title",
            "content": ". NHL Score Margin Prediction // Part 1 - Downloading Game Data and Pre-Processing . This is the first of a series of mini-projects focused on using data from NHL Stats to predict the score margin of NHL games. . toc:true- branch: master | badges: true | comments: false | author: Madhav Narendra | categories: [NHL, API, web, scraping, pandas, data, gathering, predictions, jupyter] | . Having been a long suffering fan of my hometown hockey team (Go Flames Go!), I have had some version of a hockey score / performance prediction project on the go for many, many years. In fact, when I was first self-learning programming in Python, these kinds of projects were key in helping me solidify my skills in data gathering, cleaning, and exploratory analysis. . In this series, I will implement a few different game score/margin prediction pipelines from end-to-end. Part 1 focuses on calling the NHL Stats API to download game-level regular season data for five seasons from 2014-2019, and applying some pre-processing. . . Module Imports . import requests import pandas as pd from pandas import json_normalize from datetime import datetime from time import sleep . Function Definitions . Let&#39;s get cracking with some function definitions. I&#39;m going to provide a brief overview of the function in text (in lieu of a docstring). I&#39;ll also pepper in some comments within the code where I feel it&#39;s useful. . Call NHL API . Avoid Scraping Raw HTML - If you&#39;re not familiar with web development, your first instinct when trying to scrape data from a table such as this one on NHL Stats, might be to try to download the data directly as you see it rendered in your browser. While there are plently of Python modules that will allow you to do this relatively easily, it is not the most robust approach as there is a good chance your implementation will break once the site undergoes an update which changes its layout. . Identify Data API - The better approach is to try to identify the API call which produces the data that is being displayed. To do this, use your browser&#39;s developer tools to inspect the network traffic as you load the page which contains your data (Chrome demo). With a bit of experience, spotting these data APIs becomes quite easy. But, when you&#39;re starting off, expect there to be some trial and error. . Replicate API call in Python - If you are lucky enough to spot a data API, you can replicate it in Python code by: . Right clicking on the API call in your browser&#39;s developer tools | Select Copy &gt; Copy as cURL | Navigate to this handy utility (or something similar) and paste in the copied cURL command to generate the corresponding Python requests code. | The generated Python code can now be used with the Python requests module as in the function. call_nhl is a function that: . takes mandatory input startSeason and optional input endSeason, each of which are strings in the format 20xx20yy where yy = xx + 1 (e.g. 20142015, 20152016, etc.) | outputs response, the JSON response from the API. | . def call_nhl(startSeason, endSeason=None): # Possible to call API for multiple seasons, # but if no end season is provided, set end season = start season. if not endSeason: endSeason = startSeason # Headers in the API call authenticate the requests headers = { &#39;authority&#39;: &#39;api.nhle.com&#39;, # Could cycle through different user agents using the fake-useragent module # if the API appears to be blocking repeated calls &#39;user-agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36&#39;, &#39;accept&#39;: &#39;*/*&#39;, &#39;origin&#39;: &#39;http://www.nhl.com&#39;, &#39;sec-fetch-site&#39;: &#39;cross-site&#39;, &#39;sec-fetch-mode&#39;: &#39;cors&#39;, &#39;sec-fetch-dest&#39;: &#39;empty&#39;, &#39;referer&#39;: &#39;http://www.nhl.com/&#39;, &#39;accept-language&#39;: &#39;en-US,en;q=0.9&#39;, } params = ( (&#39;isAggregate&#39;, &#39;false&#39;), (&#39;isGame&#39;, &#39;true&#39;), (&#39;sort&#39;, &#39;[{&quot;property&quot;:&quot;gameDate&quot;,&quot;direction&quot;:&quot;DESC&quot;}]&#39;), (&#39;start&#39;, &#39;0&#39;), # Setting limit = 0 returns all games for given season (&#39;limit&#39;, &#39;0&#39;), (&#39;factCayenneExp&#39;, &#39;gamesPlayed&gt;=1&#39;), # Through trial and error, gameTypeId=2 corresponds to regular season games # The f-string inserts endSeason and startSeason into the parameters (&#39;cayenneExp&#39;, f&#39;gameTypeId=2 and seasonId&lt;={endSeason} and seasonId&gt;={startSeason}&#39;), ) # Call API with given headers and parameters response = requests.get(&#39;https://api.nhle.com/stats/rest/en/team/summary&#39;, headers=headers, params=params) return response . Get Game Data . A previous version of the NHL API could only return a limited number of results, and would time out if too much data was requested. Although this no longer seems to be an issue (at least in the context of retrieving 5 seasons of game summaries) I will make individual calls to call_nhl for each season. . get_gameData is a function that: . Takes integer inputs startYear (year in YYYY format) and numSeasons | Returns a dictionary of season ID : game data pairs for numSeasons seasons starting with the season that began in startYear. | . def get_gameData(startYear, numSeasons): seasons = [f&quot;{startYear+i}{startYear+i+1}&quot; for i in range(numSeasons)] rows=0 res = {} for s in seasons: response = call_nhl(s) # Try except is probably more appropriate, # but if it ain&#39;t broke... if response: response = response.json() rows+=len(response[&#39;data&#39;]) df = pd.json_normalize(response[&#39;data&#39;]) res[s] = df print(f&quot;Number of games grabbed for {s} = {len(response[&#39;data&#39;])}. Total = {rows}&quot;) else: print(&quot;ERROR: unable to connect to NHL API&quot;) return None return res . What do we have so far? . Lets&#39;s call get_gameData for a single year and see what the result looks like: . data = get_gameData(2018, 1) print(&quot; nSummary of 2018-19 season game data: n&quot;) # pandas describe function provides an easy way of viewing # a summary of a dataframe. df = list(data.values())[0] df.columns . Number of games grabbed for 20182019 = 2542. Total = 2542 Summary of 2018-19 season game data: . Index([&#39;faceoffWinPct&#39;, &#39;gameDate&#39;, &#39;gameId&#39;, &#39;gamesPlayed&#39;, &#39;goalsAgainst&#39;, &#39;goalsAgainstPerGame&#39;, &#39;goalsFor&#39;, &#39;goalsForPerGame&#39;, &#39;homeRoad&#39;, &#39;losses&#39;, &#39;opponentTeamAbbrev&#39;, &#39;otLosses&#39;, &#39;penaltyKillNetPct&#39;, &#39;penaltyKillPct&#39;, &#39;pointPct&#39;, &#39;points&#39;, &#39;powerPlayNetPct&#39;, &#39;powerPlayPct&#39;, &#39;regulationAndOtWins&#39;, &#39;shotsAgainstPerGame&#39;, &#39;shotsForPerGame&#39;, &#39;teamFullName&#39;, &#39;teamId&#39;, &#39;ties&#39;, &#39;wins&#39;, &#39;winsInRegulation&#39;, &#39;winsInShootout&#39;], dtype=&#39;object&#39;) . In 2018, the NHL had 31 teams each of whom played 82 games for 31 x 82 = 2542 total games. So, our result looks good! . More Function Definitions . Great! So, we have a means of collecting game data. Now, to help with analysis and prediction down the road, I&#39;d like to extract a schedule of games as well as a rolling window aggregate from the raw dataset for each season. . Get Game schedule . To start, we need to produce a means of looking up a team&#39;s name by their ID. There is another endpoint on the NHL API that provides this, but since we have our season of data loaded already, let&#39;s just use that. . get_teamLU takes a pandas dataframe containing a game summaries as input and produces a dictionary of team ID : team name pairs. . def get_teamLU(df): return dict(zip(df[&#39;teamId&#39;], df[&#39;teamFullName&#39;])) . For the 2018-19 data, that produces: . teamLU = get_teamLU(df) teamLU . {1: &#39;New Jersey Devils&#39;, 2: &#39;New York Islanders&#39;, 3: &#39;New York Rangers&#39;, 4: &#39;Philadelphia Flyers&#39;, 5: &#39;Pittsburgh Penguins&#39;, 6: &#39;Boston Bruins&#39;, 7: &#39;Buffalo Sabres&#39;, 8: &#39;Montr√©al Canadiens&#39;, 9: &#39;Ottawa Senators&#39;, 10: &#39;Toronto Maple Leafs&#39;, 12: &#39;Carolina Hurricanes&#39;, 13: &#39;Florida Panthers&#39;, 14: &#39;Tampa Bay Lightning&#39;, 15: &#39;Washington Capitals&#39;, 16: &#39;Chicago Blackhawks&#39;, 17: &#39;Detroit Red Wings&#39;, 18: &#39;Nashville Predators&#39;, 19: &#39;St. Louis Blues&#39;, 20: &#39;Calgary Flames&#39;, 21: &#39;Colorado Avalanche&#39;, 22: &#39;Edmonton Oilers&#39;, 23: &#39;Vancouver Canucks&#39;, 24: &#39;Anaheim Ducks&#39;, 25: &#39;Dallas Stars&#39;, 26: &#39;Los Angeles Kings&#39;, 28: &#39;San Jose Sharks&#39;, 29: &#39;Columbus Blue Jackets&#39;, 30: &#39;Minnesota Wild&#39;, 52: &#39;Winnipeg Jets&#39;, 53: &#39;Arizona Coyotes&#39;, 54: &#39;Vegas Golden Knights&#39;} . Now, we define a function home_road which takes a game summary dataframe and the team lookup dictionary team_LU to produce four new columns: . home, road - contain hoame and road team IDs, respectively | homeName, roadName - contain hoame and road team names, respectively | . The funciton get_schedule puts it all together by applying home_road to each entry in the game summary dataframe df and then groups it by the gameId and gameDate columns to produce the final schedule: . def home_road(df, teamLU): res = {} res[&#39;home&#39;] = df[df[&#39;homeRoad&#39;]==&#39;H&#39;][&#39;teamId&#39;].values[0] res[&#39;road&#39;] = df[df[&#39;homeRoad&#39;]==&#39;R&#39;][&#39;teamId&#39;].values[0] res[&#39;homeName&#39;] = teamLU[res[&#39;home&#39;]] res[&#39;roadName&#39;] = teamLU[res[&#39;road&#39;]] return pd.Series(res, index=res.keys()) def get_schedule(df, teamLU): return df.groupby([&#39;gameId&#39;, &#39;gameDate&#39;]).apply(home_road, teamLU) . Let&#39;s see what the schedule looks like for the 2018-19 season: . get_schedule(df, teamLU).head() . home road homeName roadName . gameId gameDate . 2018020001 2018-10-03 10 | 8 | Toronto Maple Leafs | Montr√©al Canadiens | . 2018020002 2018-10-03 15 | 6 | Washington Capitals | Boston Bruins | . 2018020003 2018-10-03 23 | 20 | Vancouver Canucks | Calgary Flames | . 2018020004 2018-10-03 28 | 24 | San Jose Sharks | Anaheim Ducks | . 2018020005 2018-10-04 7 | 6 | Buffalo Sabres | Boston Bruins | . This appears to match the table from NHL Stats. Yay! . . Rolling Aggregate . One hypothesis I&#39;d like to test is that performance in the recent past (3-7 games) is far more indicative of future performance than games in the far past. So, let&#39;s write a funciton that produces a rolling aggregate of some of columns in the game summary. . rolling_aggregate takes a dataframe of game summaries and an integer window representing the window of games to consider in each aggregation: . for certain data columns such as &#39;gamesPlayed&#39;, &#39;goalsAgainst&#39;, &#39;goalsFor&#39;, &#39;points&#39;, &#39;regulationAndOtWins&#39; etc., the aggreagtion should be a sum | for columns such as &#39;goalsForPerGame&#39;, &#39;goalsAgainstPerGame&#39; etc., the aggregation should be a mean | certain others can also be summed cumulatively | . get_rolling applies rolling_aggregate to the grouped game summaries of each team to produces an aggregated dataset. . def rolling_aggregate(df, window = 3): res = {} roll_sum = [&#39;gamesPlayed&#39;, &#39;goalsAgainst&#39;, &#39;goalsFor&#39;, &#39;losses&#39;, &#39;otLosses&#39;, &#39;points&#39;, &#39;regulationAndOtWins&#39;, &#39;winsInShootout&#39;] roll_mean = [&#39;goalsForPerGame&#39;, &#39;goalsAgainstPerGame&#39;, &#39;shotsForPerGame&#39;, &#39;shotsAgainstPerGame&#39;] cumsum = [&#39;gamesPlayed&#39;, &#39;points&#39;, &#39;goalsFor&#39;, &#39;goalsAgainst&#39;] # fill and NaNs with 0 df = df.fillna(0) # aggregate by sum res = df[roll_sum].rolling(window).sum() # aggregate by mean res = res.merge(df[roll_mean].rolling(window).mean(), left_index=True, right_index = True) res.columns = [f&quot;rolling_{col}&quot; for col in res.columns] res[&#39;rolling_pointsPct&#39;] = res[&#39;rolling_points&#39;]/(window*2) # cumulative sum res[[f&quot;cum_{col}&quot; for col in cumsum]] = df[cumsum].cumsum() res.index = df[&#39;gameId&#39;] res = res[sorted(res.columns)] return res def get_rolling(df, window=3): return df.groupby([&#39;teamId&#39;]).apply(rolling_aggregate, window) . For the 2018-19 data and a game window of 7 games, get_rolling gives the following result: . get_rolling(df,3).head(10) . cum_gamesPlayed cum_goalsAgainst cum_goalsFor cum_points rolling_gamesPlayed rolling_goalsAgainst rolling_goalsAgainstPerGame rolling_goalsFor rolling_goalsForPerGame rolling_losses rolling_otLosses rolling_points rolling_pointsPct rolling_regulationAndOtWins rolling_shotsAgainstPerGame rolling_shotsForPerGame rolling_winsInShootout . teamId gameId . 1 2018021262 1 | 3 | 4 | 2 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2018021247 2 | 6 | 5 | 2 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2018021222 3 | 8 | 9 | 4 | 3.0 | 8.0 | 2.666667 | 9.0 | 3.000000 | 1.0 | 0.0 | 4.0 | 0.666667 | 2.0 | 32.333333 | 35.000000 | 0.0 | . 2018021207 4 | 11 | 11 | 5 | 3.0 | 8.0 | 2.666667 | 7.0 | 2.333333 | 1.0 | 1.0 | 3.0 | 0.500000 | 1.0 | 28.000000 | 35.000000 | 0.0 | . 2018021199 5 | 15 | 11 | 5 | 3.0 | 9.0 | 3.000000 | 6.0 | 2.000000 | 1.0 | 1.0 | 3.0 | 0.500000 | 1.0 | 30.666667 | 32.000000 | 0.0 | . 2018021170 6 | 16 | 14 | 7 | 3.0 | 8.0 | 2.666667 | 5.0 | 1.666667 | 1.0 | 1.0 | 3.0 | 0.500000 | 1.0 | 35.333333 | 25.333333 | 0.0 | . 2018021150 7 | 17 | 15 | 9 | 3.0 | 6.0 | 2.000000 | 4.0 | 1.333333 | 1.0 | 0.0 | 4.0 | 0.666667 | 1.0 | 34.333333 | 27.000000 | 1.0 | . 2018021137 8 | 22 | 16 | 9 | 3.0 | 7.0 | 2.333333 | 5.0 | 1.666667 | 1.0 | 0.0 | 4.0 | 0.666667 | 1.0 | 32.333333 | 25.000000 | 1.0 | . 2018021122 9 | 26 | 17 | 9 | 3.0 | 10.0 | 3.333333 | 3.0 | 1.000000 | 2.0 | 0.0 | 2.0 | 0.333333 | 0.0 | 24.333333 | 25.000000 | 1.0 | . 2018021111 10 | 29 | 17 | 9 | 3.0 | 12.0 | 4.000000 | 2.0 | 0.666667 | 3.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 27.666667 | 21.666667 | 0.0 | . Putting it all together . Let&#39;s wrap all the functions we defined into a single function we can all on mutiple seasons of game summary data. . process_data takes a dicttionary of game summary dataframes produced by get_gameData and outputs a dictionary containing the following: . raw dataframe, | team lookup, | game schedule, | rolling aggregate dataframe | . for each of the seasons being analysed. . def process_data(raw_data, window=3): data = {} for season, df in raw_data.items(): df[&#39;gameDate&#39;] = pd.to_datetime(df[&#39;gameDate&#39;]) df[&#39;seasonId&#39;] = str(season) df = df.sort_values(&#39;gameDate&#39;, axis=0).reset_index(drop=True) teamLU = get_teamLU(df) schedule = get_schedule(df, teamLU) rolling = get_rolling(df,window) data[season] = {&#39;raw_data&#39;:df, &#39;teamLU&#39; : teamLU, &#39;schedule&#39;:schedule, &#39;rolling&#39;:rolling, } return data . Getting data for 5 NHL seasons . So, let&#39;s get all the data we&#39;ll use in the prediction task. I&#39;m going with the 5 most recent complete NHL seasons that were played - i.e. excluding the Covid-shortened seasons of 2019-20 and 2021. . raw_data = get_gameData(2014, 5) # Process data data = process_data(raw_data) . Number of games grabbed for 20142015 = 2460. Total = 2460 Number of games grabbed for 20152016 = 2460. Total = 4920 Number of games grabbed for 20162017 = 2460. Total = 7380 Number of games grabbed for 20172018 = 2542. Total = 9922 Number of games grabbed for 20182019 = 2542. Total = 12464 . Great! So we have around 12k instances to complete the prediction task. . Here&#39;s a quick function to print the structure of the resulting object data. . def pretty(d, indent=0): for key, value in d.items(): print(&#39; t&#39; * indent + str(key)) if isinstance(value, dict): if key == &quot;teamLU&quot;: print(&#39; t&#39; * (indent+1) + f&quot;Dictionary: {len(value)}&quot;) else: pretty(value, indent+1) elif isinstance(value, pd.DataFrame): print(&#39; t&#39; * (indent+1) + f&quot;DataFrame: {value.shape}&quot;) pretty(data) . 20142015 raw_data DataFrame: (2460, 28) teamLU Dictionary: 30 schedule DataFrame: (1230, 4) rolling DataFrame: (2460, 17) 20152016 raw_data DataFrame: (2460, 28) teamLU Dictionary: 30 schedule DataFrame: (1230, 4) rolling DataFrame: (2460, 17) 20162017 raw_data DataFrame: (2460, 28) teamLU Dictionary: 30 schedule DataFrame: (1230, 4) rolling DataFrame: (2460, 17) 20172018 raw_data DataFrame: (2542, 28) teamLU Dictionary: 31 schedule DataFrame: (1271, 4) rolling DataFrame: (2542, 17) 20182019 raw_data DataFrame: (2542, 28) teamLU Dictionary: 31 schedule DataFrame: (1271, 4) rolling DataFrame: (2542, 17) . The above passes a sense check: . teamLU has 30 teams until 2017-18 which was the inaugural season of the Vegas Golden Knights (and what an inauguration that was!). | raw_data and rolling always have 82 games x N teams = 82N entries | schedule always has 82N/2 entries. | . Now that we are statisfied with the accuracy of the data, let&#39;s serialize it and save it for later - i.e. Part 2! . import pickle pickle.dump( data, open( &quot;data.p&quot;, &quot;wb&quot; ) ) .",
            "url": "https://www.madhav.me/2021/01/07/nhl_pred.html",
            "relUrl": "/2021/01/07/nhl_pred.html",
            "date": " ‚Ä¢ Jan 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://www.madhav.me/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://www.madhav.me/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://www.madhav.me/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://www.madhav.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}